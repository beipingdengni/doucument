

```python
!pip3 install nltk
import nltk
from nltk.corpus import wordnet

# 下载WordNet数据
nltk.download('wordnet')

# 查找"不合适"的同义词
synonyms = []
for syn in wordnet.synsets("suitable"):
    for lemma in syn.lemmas():
        synonyms.append(lemma.name())
# 去重并显示结果
synonyms = list(set(synonyms))
print(synonyms)
```

机器学习方式

```python
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 加载预训练的Word2Vec模型
model = Word2Vec.load("path_to_your_pretrained_model")

# 获取词向量
word1_vec = model.wv['词语1']
word2_vec = model.wv['词语2']

# 计算余弦相似度
similarity = cosine_similarity([word1_vec], [word2_vec])
print(f"词语1和词语2的相似度为: {similarity[0][0]}")
```

深度学习方式

```python
from transformers import BertTokenizer, BertModel
import torch

# 加载预训练的BERT模型和分词器
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

def get_word_embedding(word):
    inputs = tokenizer(word, return_tensors='pt')
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

# 获取词向量
word1_vec = get_word_embedding('词语1')
word2_vec = get_word_embedding('词语2')

# 计算余弦相似度
cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)
similarity = cos(word1_vec, word2_vec)
print(f"词语1和词语2的相似度为: {similarity.item()}")
```

